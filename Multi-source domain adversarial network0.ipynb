{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B51TCtsXXli1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/others/PG-MDAN'\n",
        "os.chdir(path)\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BeYReWi7UL9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64hBKDKMXjXy"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import argparse, sys, os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.init as init\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import Parameter\n",
        "import time\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjgR-Mq6XjX2"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "def setup_seed(seed):\n",
        "     torch.manual_seed(seed)\n",
        "     torch.cuda.manual_seed_all(seed)\n",
        "     np.random.seed(seed)\n",
        "     random.seed(seed)\n",
        "     torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def smape_loss_func(preds, labels):\n",
        "\n",
        "  preds = np.asarray(preds)\n",
        "  labels = np.asarray(labels)\n",
        "  mask= labels > 0\n",
        "  return np.mean(np.fabs(labels[mask]-preds[mask])/(np.fabs(labels[mask])+np.fabs(preds[mask])))\n",
        "\n",
        "def mae_loss_func(preds, labels):\n",
        "  preds = np.asarray(preds)\n",
        "  labels = np.asarray(labels)\n",
        "  mask= labels > 0\n",
        "  return np.fabs((labels[mask]-preds[mask])).mean()\n",
        "\n",
        "def nrmse_func2(preds, labels):\n",
        "\n",
        "  preds = np.asarray(preds)\n",
        "  labels = np.asarray(labels)\n",
        "  mask = labels > 0\n",
        "  return np.sqrt(np.sum((labels[mask]-preds[mask])**2)/np.sum(np.square(labels[mask])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq581dU3XjX3"
      },
      "outputs": [],
      "source": [
        "def tensorconstruct (time_slide1, time_slide2, flow, near_road, k = 6, t_p = 31, t_input = 12, t_pre = 6, num_links = 40):\n",
        "  image = []\n",
        "  for i in range(np.shape(near_road)[0]):\n",
        "    road_id = []\n",
        "    for j in range(k):\n",
        "      road_id.append(near_road[i][j])\n",
        "    image.append(flow[road_id, :])\n",
        "  image1 = np.reshape(image, [-1, k, len(flow[0,:])])\n",
        "  image2 = np.transpose(image1,(1,2,0))\n",
        "  image3 = []\n",
        "  label = []\n",
        "\n",
        "  for i in range(0,t_p):\n",
        "    for j in range(144-t_input-t_pre):\n",
        "      image3.append(image2[:, i*144+j:i*144+j+t_input, :][:])\n",
        "      label.append(flow[:, i*144+j+t_input:i*144+j+t_input+t_pre][:])\n",
        "\n",
        "  image3 = np.asarray(image3)\n",
        "  label = np.asarray(label)\n",
        "\n",
        "  image_train = image3[math.floor(np.shape(image3)[0]*time_slide1) : math.ceil(np.shape(image3)[0]*time_slide2)]\n",
        "  image_test = image3[math.floor(np.shape(image3)[0]*time_slide2):]\n",
        "  label_train = label[math.floor(np.shape(image3)[0]*time_slide1) : math.ceil(np.shape(image3)[0]*time_slide2)]\n",
        "  label_test = label[math.floor(np.shape(label)[0]*time_slide2):]\n",
        "\n",
        "\n",
        "  return image_train, image_test, label_train, label_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY08OJnCLhR9"
      },
      "outputs": [],
      "source": [
        "def series_impute (small_train, small_label, long_size):\n",
        "  train_tmp = small_train\n",
        "  label_tmp = small_label\n",
        "  for i in range (math.ceil(long_size/len(small_train))):\n",
        "    train_tmp = np.concatenate((train_tmp,small_train), axis = 0)\n",
        "    label_tmp = np.concatenate((label_tmp,small_label), axis = 0)\n",
        "\n",
        "  train_tmp = train_tmp[:long_size,:,:,:]\n",
        "  label_tmp = label_tmp[:long_size,:,:]\n",
        "\n",
        "  return train_tmp, label_tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ehj1GexKzi9"
      },
      "outputs": [],
      "source": [
        "def time_slide(time_slide1s, time_slide1t, time_slide2):\n",
        "\n",
        "  image_train_s1, image_test_s1, label_train_s1, label_test_s1 = tensorconstruct(time_slide1 = time_slide1s, time_slide2=time_slide2,\n",
        "  flow = flow_source1, near_road=near_road_source1, k = 6, t_p = 31, t_input = 12, t_pre = 6, num_links = 40)\n",
        "\n",
        "  image_train_s2, image_test_s2, label_train_s2, label_test_s2= tensorconstruct(time_slide1 = time_slide1s, time_slide2=time_slide2,\n",
        "  flow =flow_source2, near_road=near_road_source2, k = 6, t_p = 31, t_input = 12, t_pre = 6, num_links = 40)\n",
        "\n",
        "  image_train_s3, image_test_s3, label_train_s3, label_test_s3 = tensorconstruct(time_slide1 = time_slide1s, time_slide2=time_slide2,\n",
        "  flow =flow_source3, near_road=near_road_source3, k = 6, t_p = 31, t_input = 12, t_pre = 6, num_links = 40)\n",
        "\n",
        "  image_train_s4, image_test_s4, label_train_s4, label_test_s4 = tensorconstruct(time_slide1 = time_slide1s, time_slide2=time_slide2,\n",
        "  flow =flow_source4, near_road=near_road_source4, k = 6, t_p = 31, t_input = 12, t_pre = 6, num_links = 40)\n",
        "\n",
        "  image_train_t, image_test_t, label_train_t, label_test_t = tensorconstruct(time_slide1 = time_slide1t, time_slide2=time_slide2,\n",
        "  flow =flow_target, near_road=near_road_target, k = 6, t_p = 31, t_input = 12, t_pre = 6, num_links = 40)\n",
        "\n",
        "  len_t = len(image_train_t)\n",
        "  len_s = len(image_train_s1)\n",
        "  long_size = max(len_t,len_s)\n",
        "  if len_t <= len_s:\n",
        "    image_train_t,label_train_t= series_impute (image_train_t, label_train_t, long_size)\n",
        "\n",
        "  image_train_s1 = image_train_s1.reshape(-1, 40, 6, 12)\n",
        "  image_train_s2 = image_train_s2.reshape(-1, 40, 6, 12)\n",
        "  image_train_s3 = image_train_s3.reshape(-1, 40, 6, 12)\n",
        "  image_train_s4 = image_train_s4.reshape(-1, 40, 6, 12)\n",
        "  image_train_t = image_train_t.reshape(-1, 40, 6, 12)\n",
        "\n",
        "  image_test_s1 = image_test_s1.reshape(-1, 40, 6, 12)\n",
        "  image_test_s2 = image_test_s2.reshape(-1, 40, 6, 12)\n",
        "  image_test_s3 = image_test_s3.reshape(-1, 40, 6, 12)\n",
        "  image_test_s4 = image_test_s4.reshape(-1, 40, 6, 12)\n",
        "  image_test_t = image_test_t.reshape(-1, 40, 6, 12)\n",
        "\n",
        "  test = image_test_t\n",
        "  label_test = label_test_t\n",
        "\n",
        "  return image_train_s1, image_test_s1, label_train_s1, label_test_s1, image_train_s2, image_test_s2, label_train_s2, label_test_s2, image_train_s3, image_test_s3, label_train_s3, label_test_s3, image_train_s4, image_test_s4, label_train_s4, label_test_s4, image_train_t, image_test_t, label_train_t, label_test_t, test, label_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26GaOfX7XjX5"
      },
      "outputs": [],
      "source": [
        "def get_train_loader(data,label,batch_size,shuffle=False):\n",
        "  \"\"\"\n",
        "  Get train dataloader of source domain or target domain\n",
        "  :return: dataloader\n",
        "  \"\"\"\n",
        "  tensor_x = torch.Tensor(data) # transform to torch tensor\n",
        "  tensor_y = torch.Tensor(label)\n",
        "\n",
        "  my_dataset = torch.utils.data.TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "  my_dataloader = torch.utils.data.DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle,drop_last=True,pin_memory=True) # create your dataloader\n",
        "\n",
        "  return my_dataloader\n",
        "\n",
        "def get_test_loader(data,label,batch_size,shuffle=False):\n",
        "  \"\"\"\n",
        "  Get test dataloader of source domain or target domain\n",
        "  :return: dataloader\n",
        "  \"\"\"\n",
        "  tensor_x = torch.Tensor(data) # transform to torch tensor\n",
        "  tensor_y = torch.Tensor(label)\n",
        "\n",
        "  my_dataset = torch.utils.data.TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "  my_dataloader = torch.utils.data.DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle,drop_last=True,pin_memory=True) # create your dataloader\n",
        "\n",
        "  return my_dataloader\n",
        "\n",
        "\n",
        "def optimizer_scheduler(optimizer, p):\n",
        "  \"\"\"\n",
        "  Adjust the learning rate of optimizer\n",
        "  :param optimizer: optimizer for updating parameters\n",
        "  :param p: a variable for adjusting learning rate\n",
        "  :return: optimizer\n",
        "  \"\"\"\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = 0.01 / (1. + 10 * p) ** 0.75\n",
        "\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOALwB7MXjX6"
      },
      "outputs": [],
      "source": [
        "class GradReverse(torch.autograd.Function):\n",
        "  \"\"\"\n",
        "  Extension of grad reverse layer\n",
        "  \"\"\"\n",
        "  @staticmethod\n",
        "  def forward(ctx, x, constant):\n",
        "    ctx.constant = constant\n",
        "    return x.view_as(x)\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    grad_output = grad_output.neg() * ctx.constant\n",
        "    return grad_output, None\n",
        "\n",
        "  def grad_reverse(x, constant):\n",
        "    return GradReverse.apply(x, constant)\n",
        "\n",
        "class Extractor(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Extractor, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels = 40, out_channels = 30, kernel_size = 3, stride = 1,  padding = 1)\n",
        "    self.bn1 = nn.BatchNorm2d(30)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 30, out_channels = 30, kernel_size = 3, stride = 1,  padding = 1)\n",
        "    self.bn2 = nn.BatchNorm2d(30)\n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "    x = F.relu(self.bn1(self.conv1(input)))\n",
        "    x = F.relu(self.bn2(self.conv2(x)))\n",
        "    # x = self.pool1(x)\n",
        "    x = torch.reshape(x, (-1, 30*6*12))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class Predictor(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Predictor, self).__init__()\n",
        "    self.fc1 = nn.Linear(30*6*12, 40*6)\n",
        "\n",
        "  def forward(self, input):\n",
        "    pre = F.relu(self.fc1(input))\n",
        "\n",
        "    pre = pre.reshape(-1,40,6)\n",
        "\n",
        "    return pre\n",
        "\n",
        "class Domain_classifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Domain_classifier, self).__init__()\n",
        "    # self.fc1 = nn.Linear(50 * 4 * 4, 100)\n",
        "    # self.bn1 = nn.BatchNorm1d(100)\n",
        "    # self.fc2 = nn.Linear(100, 2)\n",
        "    self.fc1 = nn.Linear(30*6*12, 1024)\n",
        "    self.fc2 = nn.Linear(1024, 2)\n",
        "\n",
        "  def forward(self, input, constant):\n",
        "    input = GradReverse.grad_reverse(input, constant)\n",
        "    # logits = F.relu(self.bn1(self.fc1(input)))\n",
        "    # logits = F.log_softmax(self.fc2(logits), 1)\n",
        "    logits = F.relu(self.fc1(input))\n",
        "\n",
        "    logits = F.log_softmax(self.fc2(logits), dim = 1)\n",
        "\n",
        "    return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrpKAdhBXjX7"
      },
      "outputs": [],
      "source": [
        "def train(training_mode, feature_extractor, class_classifier, domain_classifier, class_criterion, domain_criterion,\n",
        "        source_dataloader1, source_dataloader2, source_dataloader3, source_dataloader4, target_dataloader,val_dataloader, optimizer, epoches):\n",
        "  for epoch in range(epoches):\n",
        "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
        "    # track the losses as the model trains\n",
        "    train_losses,valid_losses,avg_train_losses,avg_valid_losses = [],[],[],[]\n",
        "    # steps\n",
        "    start_steps = epoch * len(source_dataloader1)\n",
        "    total_steps = 10 * len(source_dataloader1)\n",
        "\n",
        "    if training_mode == 'Pre-train':\n",
        "      print('Pre-train Epoch: {}'.format(epoch))\n",
        "      # setup models\n",
        "      feature_extractor.train()\n",
        "      class_classifier.train()\n",
        "      domain_classifier.train()\n",
        "      for batch_idx, (sdata1, sdata2, sdata3, sdata4, tdata) in enumerate(zip(source_dataloader1, source_dataloader2,\n",
        "      source_dataloader3, source_dataloader4, target_dataloader)):\n",
        "\n",
        "        # setup hyperparameters\n",
        "        p = float(batch_idx + start_steps) / total_steps\n",
        "        constant = 2. / (1. + np.exp(-gamma * p)) - 1\n",
        "\n",
        "        # prepare the data\n",
        "        input1s, label1s= sdata1\n",
        "        input2s, label2s= sdata2\n",
        "        input3s, label3s= sdata3\n",
        "        input4s, label4s= sdata4\n",
        "        input2, label2= tdata\n",
        "\n",
        "        size = min((input1s.shape[0], input2.shape[0]))\n",
        "        input1s, label1s = input1s[0:size, :, :], label1s[0:size]\n",
        "        input2s, label2s = input2s[0:size, :, :], label2s[0:size]\n",
        "        input3s, label3s = input3s[0:size, :, :], label3s[0:size]\n",
        "        input4s, label4s = input4s[0:size, :, :], label4s[0:size]\n",
        "        input2, label2 = input2[0:size, :, :], label2[0:size]\n",
        "\n",
        "        input1s, label1s= Variable(input1s.to(device)), Variable(label1s.to(device).float())\n",
        "        input2s, label2s= Variable(input2s.to(device)), Variable(label2s.to(device).float())\n",
        "        input3s, label3s= Variable(input3s.to(device)), Variable(label3s.to(device).float())\n",
        "        input4s, label4s= Variable(input4s.to(device)), Variable(label4s.to(device).float())\n",
        "        input2, label2= Variable(input2.to(device)), Variable(label2.to(device).float())\n",
        "\n",
        "        # setup optimizer\n",
        "        optimizer = optimizer_scheduler(optimizer, p)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # prepare domain labels\n",
        "        source_label1s = Variable(torch.zeros((input1s.size()[0])).type(torch.FloatTensor).to(device))\n",
        "        source_label2s = Variable(torch.zeros((input2s.size()[0])).type(torch.FloatTensor).to(device))\n",
        "        source_label3s = Variable(torch.zeros((input3s.size()[0])).type(torch.FloatTensor).to(device))\n",
        "        source_label4s = Variable(torch.zeros((input4s.size()[0])).type(torch.FloatTensor).to(device))\n",
        "\n",
        "        target_labels = Variable(torch.ones((input2.size()[0])).type(torch.FloatTensor).to(device))\n",
        "\n",
        "        # compute the output of source domain and target domain\n",
        "        src_feature1 = feature_extractor(input1s)\n",
        "        src_feature2 = feature_extractor(input2s)\n",
        "        src_feature3 = feature_extractor(input3s)\n",
        "        src_feature4 = feature_extractor(input4s)\n",
        "        tgt_feature = feature_extractor(input2)\n",
        "\n",
        "        # compute the loss of source & target preds\n",
        "        class_pred1s = class_classifier(src_feature1)\n",
        "        class_pred2s = class_classifier(src_feature2)\n",
        "        class_pred3s = class_classifier(src_feature3)\n",
        "        class_pred4s = class_classifier(src_feature4)\n",
        "        class_predt = class_classifier(tgt_feature)\n",
        "\n",
        "        class_loss1s = class_criterion(class_pred1s, label1s)\n",
        "        class_loss2s = class_criterion(class_pred2s, label2s)\n",
        "        class_loss3s = class_criterion(class_pred3s, label3s)\n",
        "        class_loss4s = class_criterion(class_pred4s, label4s)\n",
        "        class_losst = class_criterion(class_predt, label2)\n",
        "\n",
        "        # compute the domain loss of src_feature and target_feature\n",
        "        tgt_preds = domain_classifier(tgt_feature, constant)\n",
        "        src_preds1 = domain_classifier(src_feature1, constant)\n",
        "        src_preds2 = domain_classifier(src_feature2, constant)\n",
        "        src_preds3 = domain_classifier(src_feature3, constant)\n",
        "        src_preds4 = domain_classifier(src_feature4, constant)\n",
        "\n",
        "        tgt_loss = domain_criterion(tgt_preds.float(), target_labels.long())\n",
        "        src_loss1 = domain_criterion(src_preds1.float(), source_label1s.long())\n",
        "        src_loss2 = domain_criterion(src_preds2.float(), source_label2s.long())\n",
        "        src_loss3 = domain_criterion(src_preds3.float(), source_label3s.long())\n",
        "        src_loss4 = domain_criterion(src_preds4.float(), source_label4s.long())\n",
        "\n",
        "        domain_loss = 1*tgt_loss + 0.37*src_loss1 + 0.06*src_loss2 + 0.31*src_loss3 + 0.26*src_loss4\n",
        "        class_loss = class_losst + 0.37*class_loss1s + 0.06*class_loss2s + 0.31*class_loss3s + 0.26*class_loss4s\n",
        "\n",
        "        # domain_loss = 1*tgt_loss + 1*src_loss1 + 0*src_loss2 + 1*src_loss3 + 0*src_loss4\n",
        "        # class_loss = 1*class_losst + 1*class_loss1s + 0*class_loss2s + 1*class_loss3s + 0*class_loss4s\n",
        "\n",
        "        class_loss = class_loss/(5*batch_size*40*6)\n",
        "\n",
        "        loss = (1-theta)*class_loss + theta * domain_loss\n",
        "\n",
        "        loss=loss.float()\n",
        "  #             print(loss.dtype)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # print loss\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print('[{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tPred Loss: {:.6f}\\tDomain Loss: {:.6f}'.format(\n",
        "              batch_idx * len(input2), len(target_dataloader.dataset),\n",
        "              100. * batch_idx / len(target_dataloader), loss.item(), class_loss.item(),\n",
        "              domain_loss.item()\n",
        "            ))\n",
        "\n",
        "            total_loss.append(loss.item())\n",
        "            c_loss.append(class_loss.item())\n",
        "            d_loss.append(domain_loss.item())\n",
        "\n",
        "\n",
        "    elif training_mode == 'Fine-tune':\n",
        "      print('FT Epoch: {}'.format(epoch))\n",
        "      # setup models\n",
        "      feature_extractor.train()\n",
        "      class_classifier.train()\n",
        "      domain_classifier.train()\n",
        "      for batch_idx,tdata in enumerate(target_dataloader):\n",
        "        p = float(batch_idx + start_steps) / total_steps\n",
        "        constant = 2. / (1. + np.exp(-gamma * p)) - 1\n",
        "        #prepare the target data\n",
        "        input2, label2, = tdata\n",
        "\n",
        "        # print(input2.shape)\n",
        "        # print(input2.type)\n",
        "        input2, label2, = Variable(input2.to(device)), Variable(label2.to(device).float())\n",
        "\n",
        "        # setup optimizer\n",
        "        optimizer = optimizer_scheduler(optimizer, p)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # compute the output of target domain\n",
        "        tgt_feature = feature_extractor(input2)\n",
        "\n",
        "        # compute the loss of target preds\n",
        "        class_predt = class_classifier(tgt_feature)\n",
        "\n",
        "        class_losst = class_criterion(class_predt, label2)\n",
        "\n",
        "        loss = class_losst/(batch_size*40*6)\n",
        "        loss=loss.float()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # print loss\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "          print('[({:.0f}%)]\\tPreds Loss: {:.6f}'.format(\n",
        "            100. * batch_idx / len(target_dataloader), loss.item()))\n",
        "      # val\n",
        "      feature_extractor.eval()\n",
        "      class_classifier.eval()\n",
        "      for batch_idx,vdata in enumerate(val_dataloader):\n",
        "        input_val, label_val, = vdata\n",
        "        input_val, label_val, = Variable(input_val.to(device)), Variable(label_val.to(device).float())\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        tgt_feature_val = feature_extractor(input_val)\n",
        "        # compute the loss of target preds\n",
        "        class_pred_val = class_classifier(tgt_feature_val)\n",
        "\n",
        "        # calculate the loss\n",
        "        loss_val = class_criterion(class_pred_val, label_val)\n",
        "        # record validation loss\n",
        "        valid_losses.append(loss_val.item())\n",
        "\n",
        "      train_loss = np.average(train_losses)\n",
        "      valid_loss = np.average(valid_losses)\n",
        "      avg_train_losses.append(train_loss)\n",
        "      avg_valid_losses.append(valid_loss)\n",
        "\n",
        "      train_losses = []\n",
        "      valid_losses = []\n",
        "      # early_stopping needs the validation loss to check if it has decresed,\n",
        "      # and if it has, it will make a checkpoint of the current model\n",
        "      early_stopping(valid_loss, class_classifier)\n",
        "\n",
        "      if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzwrFYnUXjX8"
      },
      "outputs": [],
      "source": [
        "def test1(feature_extractor, class_classifier, domain_classifier,target_dataloader):\n",
        "  \"\"\"\n",
        "  Test the performance of the model\n",
        "  :param feature_extractor: network used to extract feature from target samples\n",
        "  :param class_classifier: network used to predict labels\n",
        "  :param domain_classifier: network used to predict domain\n",
        "  :param source_dataloader: test dataloader of source domain\n",
        "  :param target_dataloader: test dataloader of target domain\n",
        "  :return: None\n",
        "  \"\"\"\n",
        "  feature_extractor.eval()\n",
        "  class_classifier.eval()\n",
        "  domain_classifier.eval()\n",
        "  target_correct = 0.0\n",
        "  domain_correct = 0.0\n",
        "  tgt_correct = 0.0\n",
        "\n",
        "  pred_list_t = []\n",
        "  label_list_t = []\n",
        "  for batch_idx, tdata in enumerate(target_dataloader):\n",
        "    # setup hyperparameters\n",
        "    p = float(batch_idx) / len(target_dataloader)\n",
        "    constant = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "\n",
        "    inputt, labelt= tdata\n",
        "    inputt, labelt= Variable(inputt.to(device)), Variable(labelt.to(device).float())\n",
        "    tgt_labels = Variable(torch.ones((inputt.size()[0])).type(torch.FloatTensor).to(device))\n",
        "\n",
        "    outputt = class_classifier(feature_extractor(inputt))\n",
        "    predt = outputt\n",
        "    pred_list_t.append(predt.tolist())\n",
        "    label_list_t.append(labelt.tolist())\n",
        "\n",
        "    tgt_preds = domain_classifier(feature_extractor(inputt), constant)\n",
        "    tgt_preds = tgt_preds.data.max(1, keepdim=True)[1]\n",
        "    tgt_correct += tgt_preds.eq(tgt_labels.data.view_as(tgt_preds)).cuda().sum()\n",
        "\n",
        "\n",
        "\n",
        "  target_nrmse = nrmse_func2(pred_list_t, label_list_t)\n",
        "  target_smape = smape_loss_func(pred_list_t, label_list_t)\n",
        "  target_mae = mae_loss_func(pred_list_t, label_list_t)\n",
        "\n",
        "  print('\\nTarget error: {}/{}/{} \\n'.format(target_nrmse, target_smape, target_mae))\n",
        "  t_nrmse_list.append(target_nrmse)\n",
        "  t_smape_list.append(target_smape)\n",
        "  t_mae_list.append(target_mae)\n",
        "  return t_mae_list,t_smape_list,t_nrmse_list\n",
        "  # np.save(file = r'.\\preds\\nrmse=%.4f, mae=%.4f, smape=%.4f' %(target_nrmse, target_mae, target_smape), arr = pred_list_t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNJ6-waJXjX9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  # prepare the source data and target data\n",
        "  image_train_s1, image_test_s1, label_train_s1, label_test_s1, image_train_s2, image_test_s2, label_train_s2, label_test_s2, image_train_s3, image_test_s3, label_train_s3, label_test_s3, image_train_s4, image_test_s4, label_train_s4, label_test_s4, image_train_t,image_test_t,label_train_t,label_test_t,test,label_test= time_slide(time_slide1s=0/31, time_slide1t=22/31, time_slide2=25/31)\n",
        "\n",
        "  print(label_train_s1.shape)\n",
        "  time01 = time.time()\n",
        "  val_num=144\n",
        "  src_train_dataloader1 = get_train_loader(image_train_s1,label_train_s1,batch_size=batch_size,shuffle=True)\n",
        "  src_test_dataloader1 = get_test_loader(image_test_s1,label_test_s1,batch_size=batch_size,shuffle=True)\n",
        "  src_train_dataloader2 = get_train_loader(image_train_s2,label_train_s2,batch_size=batch_size,shuffle=True)\n",
        "  src_test_dataloader2 = get_test_loader(image_test_s2,label_test_s2,batch_size=batch_size,shuffle=True)\n",
        "  src_train_dataloader3 = get_train_loader(image_train_s3,label_train_s3,batch_size=batch_size,shuffle=True)\n",
        "  src_test_dataloader3 = get_test_loader(image_test_s3,label_test_s3,batch_size=batch_size,shuffle=True)\n",
        "  src_train_dataloader4 = get_train_loader(image_train_s4,label_train_s4,batch_size=batch_size,shuffle=True)\n",
        "  src_test_dataloader4 = get_test_loader(image_test_s4,label_test_s4,batch_size=batch_size,shuffle=True)\n",
        "\n",
        "  tgt_train_dataloader = get_train_loader(image_train_t,label_train_t,batch_size=batch_size,shuffle=True)\n",
        "  tgt_test_dataloader = get_test_loader(image_test_t,label_test_t,batch_size=batch_size,shuffle=True)\n",
        "  val_train_dataloader = get_train_loader(image_test_t[:val_num,:,:,:],label_test_t[:val_num,:,:],batch_size=batch_size,shuffle=True)\n",
        "\n",
        "  time02 = time.time()\n",
        "\n",
        "  print('data_process_time: '+ str (time02-time01))\n",
        "\n",
        "  # init models\n",
        "  feature_extractor = Extractor().to(device)\n",
        "  class_classifier = Predictor().to(device)\n",
        "  domain_classifier = Domain_classifier().to(device)\n",
        "\n",
        "  # init criterions\n",
        "  class_criterion = nn.MSELoss().to(device)\n",
        "  # domain_criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "  domain_criterion = nn.NLLLoss()\n",
        "\n",
        "  # init optimizer\n",
        "  optimizer = optim.Adam([\n",
        "          {'params': feature_extractor.parameters()},\n",
        "                          {'params': class_classifier.parameters()},\n",
        "                          {'params': domain_classifier.parameters()}\n",
        "  ], lr= 0.001)\n",
        "\n",
        "  time1 = time.time()\n",
        "\n",
        "  train('Pre-train', feature_extractor, class_classifier, domain_classifier, class_criterion, domain_criterion,\n",
        "    src_train_dataloader1, src_train_dataloader2, src_train_dataloader3, src_train_dataloader4, tgt_train_dataloader,val_train_dataloader, optimizer, epoches=50)\n",
        "  test1(feature_extractor, class_classifier, domain_classifier,tgt_test_dataloader)\n",
        "\n",
        "  time2 = time.time()\n",
        "  train('Fine-tune', feature_extractor, class_classifier, domain_classifier, class_criterion, domain_criterion,\n",
        "    src_train_dataloader1, src_train_dataloader2, src_train_dataloader3, src_train_dataloader4, tgt_train_dataloader,val_train_dataloader, optimizer, epoches=40)\n",
        "  test1(feature_extractor, class_classifier, domain_classifier,tgt_test_dataloader)\n",
        "\n",
        "  time3 = time.time()\n",
        "  print('pretraining time: ' + str(time2-time1))\n",
        "  print('finetuning time: ' + str(time3-time2))\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "total_loss, d_loss, c_loss = [],[],[]\n",
        "s1_nrmse_list, s2_nrmse_list, s3_nrmse_list, s4_nrmse_list, t_nrmse_list, domain_loss_list = [],[],[],[],[],[]\n",
        "s1_smape_list, s2_smape_list, s3_smape_list, s4_smape_list, t_smape_list = [],[],[],[],[]\n",
        "s1_mae_list, s2_mae_list, s3_mae_list, s4_mae_list, t_mae_list = [],[],[],[],[]\n",
        "near_road_target = np.argsort(np.array(pd.read_csv('./data/dis_blue.csv',header = None)))\n",
        "flow_target = np.array(pd.read_csv('./data/flow_blue.csv', header= 0))\n",
        "\n",
        "near_road_source1 = np.argsort(np.array(pd.read_csv('./data/dis_green.csv',header = None)))\n",
        "flow_source1 = np.array(pd.read_csv('./data/flow_green.csv', header= 0))\n",
        "\n",
        "near_road_source2 = np.argsort(np.array(pd.read_csv('./data/dis_yellow.csv',header = None)))\n",
        "flow_source2 = np.array(pd.read_csv('./data/flow_yellow.csv', header= 0))\n",
        "\n",
        "near_road_source3 = np.argsort(np.array(pd.read_csv('./data/dis_purple.csv',header = None)))\n",
        "flow_source3 = np.array(pd.read_csv('./data/flow_purple.csv', header= 0))\n",
        "\n",
        "near_road_source4 = np.argsort(np.array(pd.read_csv('./data/dis_red.csv',header = None)))\n",
        "flow_source4 = np.array(pd.read_csv('./data/flow_red.csv', header= 0))\n",
        "# pred_list_s1, pred_list_s2, pred_list_s3, pred_list_s4, pred_list_t = [],[],[],[],[]\n",
        "# label_list_s1, label_list_s2, label_list_s3, label_list_s4, label_list_t = [],[],[],[],[]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  setup_seed(0)\n",
        "  target_links=40\n",
        "  gamma = 5\n",
        "  theta = 0.05\n",
        "  batch_size = 64\n",
        "  time_start=time.time()\n",
        "  main()\n",
        "  time_end=time.time()\n",
        "  print('total run time: (min)',(time_end-time_start)/60.)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "ede0b0c248aff89f28ae18209387c18b6576badc4d44ab9abec62ea9812901de"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "ede0b0c248aff89f28ae18209387c18b6576badc4d44ab9abec62ea9812901de"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
