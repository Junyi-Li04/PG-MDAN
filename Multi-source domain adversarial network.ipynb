{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"B51TCtsXXli1","executionInfo":{"status":"ok","timestamp":1715767261434,"user_tz":-120,"elapsed":535,"user":{"displayName":"J Lee","userId":"09182393954667450926"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0252e7b9-8e18-495d-8a9c-96729d116ade"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/others/PG-MDAN\n"]}],"source":["import os\n","path = '/content/drive/MyDrive/Colab Notebooks/others/PG-MDAN'\n","os.chdir(path)\n","print(os.getcwd())"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BeYReWi7UL9n","executionInfo":{"status":"ok","timestamp":1715767259467,"user_tz":-120,"elapsed":21336,"user":{"displayName":"J Lee","userId":"09182393954667450926"}},"outputId":"1fd0b108-2824-4120-85ed-857f791c11fd"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"64hBKDKMXjXy","executionInfo":{"status":"ok","timestamp":1715767271665,"user_tz":-120,"elapsed":4202,"user":{"displayName":"J Lee","userId":"09182393954667450926"}}},"outputs":[],"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","import torch.optim as optim\n","import pandas as pd\n","import numpy as np\n","import math\n","import random\n","import argparse, sys, os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torch.nn.init as init\n","from torch.utils.data import DataLoader\n","from torch.nn import Parameter\n","import time\n","from collections import Counter\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"vjgR-Mq6XjX2","executionInfo":{"status":"ok","timestamp":1715767271665,"user_tz":-120,"elapsed":5,"user":{"displayName":"J Lee","userId":"09182393954667450926"}}},"outputs":[],"source":["class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","\n","    def __call__(self, val_loss, model):\n","        score = -val_loss\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss\n","\n","def setup_seed(seed):\n","     torch.manual_seed(seed)\n","     torch.cuda.manual_seed_all(seed)\n","     np.random.seed(seed)\n","     random.seed(seed)\n","     torch.backends.cudnn.deterministic = True\n","\n","def smape_loss_func(preds, labels):\n","\n","  preds = np.asarray(preds)\n","  labels = np.asarray(labels)\n","  mask= labels > 0\n","  return np.mean(np.fabs(labels[mask]-preds[mask])/(np.fabs(labels[mask])+np.fabs(preds[mask])))\n","\n","def mae_loss_func(preds, labels):\n","  preds = np.asarray(preds)\n","  labels = np.asarray(labels)\n","  mask= labels > 0\n","  return np.fabs((labels[mask]-preds[mask])).mean()\n","\n","def nrmse_func2(preds, labels):\n","\n","  preds = np.asarray(preds)\n","  labels = np.asarray(labels)\n","  mask = labels > 0\n","  return np.sqrt(np.sum((labels[mask]-preds[mask])**2)/np.sum(np.square(labels[mask])))"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"kq581dU3XjX3","executionInfo":{"status":"ok","timestamp":1715767271666,"user_tz":-120,"elapsed":5,"user":{"displayName":"J Lee","userId":"09182393954667450926"}}},"outputs":[],"source":["def tensorconstruct (time_slide1, time_slide2, flow, near_road, k = 6, t_p = 31, t_input = 12, t_pre = 6, num_links = 40):\n","  image = []\n","  for i in range(np.shape(near_road)[0]):\n","    road_id = []\n","    for j in range(k):\n","      road_id.append(near_road[i][j])\n","    image.append(flow[road_id, :])\n","  image1 = np.reshape(image, [-1, k, len(flow[0,:])])\n","  image2 = np.transpose(image1,(1,2,0))\n","  image3 = []\n","  label = []\n","\n","  for i in range(0,t_p):\n","    for j in range(144-t_input-t_pre):\n","      image3.append(image2[:, i*144+j:i*144+j+t_input, :][:])\n","      label.append(flow[:, i*144+j+t_input:i*144+j+t_input+t_pre][:])\n","\n","  image3 = np.asarray(image3)\n","  label = np.asarray(label)\n","\n","  #划分前80%数据为训练集，最后7days数据为测试集\n","  image_train = image3[math.floor(np.shape(image3)[0]*time_slide1) : math.ceil(np.shape(image3)[0]*time_slide2)]\n","  image_test = image3[math.floor(np.shape(image3)[0]*time_slide2):]\n","  label_train = label[math.floor(np.shape(image3)[0]*time_slide1) : math.ceil(np.shape(image3)[0]*time_slide2)]\n","  label_test = label[math.floor(np.shape(label)[0]*time_slide2):]\n","\n","\n","  return image_train, image_test, label_train, label_test"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"PDSF5f_NXjX4","executionInfo":{"status":"ok","timestamp":1715767271666,"user_tz":-120,"elapsed":5,"user":{"displayName":"J Lee","userId":"09182393954667450926"}}},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"vY08OJnCLhR9","executionInfo":{"status":"ok","timestamp":1715767271666,"user_tz":-120,"elapsed":4,"user":{"displayName":"J Lee","userId":"09182393954667450926"}}},"outputs":[],"source":["def series_impute (small_train, small_label, long_size):\n","  train_tmp = small_train\n","  label_tmp = small_label\n","  for i in range (math.ceil(long_size/len(small_train))):\n","    train_tmp = np.concatenate((train_tmp,small_train), axis = 0)\n","    label_tmp = np.concatenate((label_tmp,small_label), axis = 0)\n","\n","  train_tmp = train_tmp[:long_size,:,:,:]\n","  label_tmp = label_tmp[:long_size,:,:]\n","\n","  return train_tmp, label_tmp"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"5Ehj1GexKzi9","executionInfo":{"status":"ok","timestamp":1715767271666,"user_tz":-120,"elapsed":4,"user":{"displayName":"J Lee","userId":"09182393954667450926"}}},"outputs":[],"source":["def time_slide(time_slide1s, time_slide1t, time_slide2):\n","\n","  image_train_s1, image_test_s1, label_train_s1, label_test_s1 = tensorconstruct(time_slide1 = time_slide1s, time_slide2=time_slide2,\n","  flow = flow_source1, near_road=near_road_source1, k = 6, t_p = 31, t_input = 12, t_pre = 6, num_links = 40)\n","\n","  image_train_s2, image_test_s2, label_train_s2, label_test_s2= tensorconstruct(time_slide1 = time_slide1s, time_slide2=time_slide2,\n","  flow =flow_source2, near_road=near_road_source2, k = 6, t_p = 31, t_input = 12, t_pre = 6, num_links = 40)\n","\n","  image_train_s3, image_test_s3, label_train_s3, label_test_s3 = tensorconstruct(time_slide1 = time_slide1s, time_slide2=time_slide2,\n","  flow =flow_source3, near_road=near_road_source3, k = 6, t_p = 31, t_input = 12, t_pre = 6, num_links = 40)\n","\n","  image_train_s4, image_test_s4, label_train_s4, label_test_s4 = tensorconstruct(time_slide1 = time_slide1s, time_slide2=time_slide2,\n","  flow =flow_source4, near_road=near_road_source4, k = 6, t_p = 31, t_input = 12, t_pre = 6, num_links = 40)\n","\n","  image_train_t, image_test_t, label_train_t, label_test_t = tensorconstruct(time_slide1 = time_slide1t, time_slide2=time_slide2,\n","  flow =flow_target, near_road=near_road_target, k = 6, t_p = 31, t_input = 12, t_pre = 6, num_links = 40)\n","\n","  len_t = len(image_train_t)\n","  len_s = len(image_train_s1)\n","  long_size = max(len_t,len_s)\n","  if len_t <= len_s:\n","    image_train_t,label_train_t= series_impute (image_train_t, label_train_t, long_size)\n","\n","  image_train_s1 = image_train_s1.reshape(-1, 40, 6, 12)\n","  image_train_s2 = image_train_s2.reshape(-1, 40, 6, 12)\n","  image_train_s3 = image_train_s3.reshape(-1, 40, 6, 12)\n","  image_train_s4 = image_train_s4.reshape(-1, 40, 6, 12)\n","  image_train_t = image_train_t.reshape(-1, 40, 6, 12)\n","\n","  image_test_s1 = image_test_s1.reshape(-1, 40, 6, 12)\n","  image_test_s2 = image_test_s2.reshape(-1, 40, 6, 12)\n","  image_test_s3 = image_test_s3.reshape(-1, 40, 6, 12)\n","  image_test_s4 = image_test_s4.reshape(-1, 40, 6, 12)\n","  image_test_t = image_test_t.reshape(-1, 40, 6, 12)\n","\n","  test = image_test_t\n","  label_test = label_test_t\n","\n","  return image_train_s1, image_test_s1, label_train_s1, label_test_s1, image_train_s2, image_test_s2, label_train_s2, label_test_s2, image_train_s3, image_test_s3, label_train_s3, label_test_s3, image_train_s4, image_test_s4, label_train_s4, label_test_s4, image_train_t, image_test_t, label_train_t, label_test_t, test, label_test"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"7YvlJ7XwHFyu","executionInfo":{"status":"ok","timestamp":1715767271666,"user_tz":-120,"elapsed":4,"user":{"displayName":"J Lee","userId":"09182393954667450926"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"26GaOfX7XjX5","executionInfo":{"status":"ok","timestamp":1715767272143,"user_tz":-120,"elapsed":481,"user":{"displayName":"J Lee","userId":"09182393954667450926"}}},"outputs":[],"source":["def get_train_loader(data,label,batch_size,shuffle=False):\n","  \"\"\"\n","  Get train dataloader of source domain or target domain\n","  :return: dataloader\n","  \"\"\"\n","  tensor_x = torch.Tensor(data) # transform to torch tensor\n","  tensor_y = torch.Tensor(label)\n","\n","  my_dataset = torch.utils.data.TensorDataset(tensor_x,tensor_y) # create your datset\n","  my_dataloader = torch.utils.data.DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle,drop_last=True,pin_memory=True) # create your dataloader\n","\n","  return my_dataloader\n","\n","def get_test_loader(data,label,batch_size,shuffle=False):\n","  \"\"\"\n","  Get test dataloader of source domain or target domain\n","  :return: dataloader\n","  \"\"\"\n","  tensor_x = torch.Tensor(data) # transform to torch tensor\n","  tensor_y = torch.Tensor(label)\n","\n","  my_dataset = torch.utils.data.TensorDataset(tensor_x,tensor_y) # create your datset\n","  my_dataloader = torch.utils.data.DataLoader(my_dataset,batch_size=batch_size,shuffle=shuffle,drop_last=True,pin_memory=True) # create your dataloader\n","\n","  return my_dataloader\n","\n","\n","def optimizer_scheduler(optimizer, p):\n","  \"\"\"\n","  Adjust the learning rate of optimizer\n","  :param optimizer: optimizer for updating parameters\n","  :param p: a variable for adjusting learning rate\n","  :return: optimizer\n","  \"\"\"\n","  for param_group in optimizer.param_groups:\n","    param_group['lr'] = 0.01 / (1. + 10 * p) ** 0.75\n","\n","  return optimizer"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"DOALwB7MXjX6","executionInfo":{"status":"ok","timestamp":1715767272143,"user_tz":-120,"elapsed":5,"user":{"displayName":"J Lee","userId":"09182393954667450926"}}},"outputs":[],"source":["class GradReverse(torch.autograd.Function):\n","  \"\"\"\n","  Extension of grad reverse layer\n","  \"\"\"\n","  @staticmethod\n","  def forward(ctx, x, constant):\n","    ctx.constant = constant\n","    return x.view_as(x)\n","\n","  @staticmethod\n","  def backward(ctx, grad_output):\n","    grad_output = grad_output.neg() * ctx.constant\n","    return grad_output, None\n","\n","  def grad_reverse(x, constant):\n","    return GradReverse.apply(x, constant)\n","\n","class Extractor(nn.Module):\n","\n","  def __init__(self):\n","    super(Extractor, self).__init__()\n","\n","    self.conv1 = nn.Conv2d(in_channels = 40, out_channels = 30, kernel_size = 3, stride = 1,  padding = 1)\n","    self.bn1 = nn.BatchNorm2d(30)\n","    self.conv2 = nn.Conv2d(in_channels = 30, out_channels = 30, kernel_size = 3, stride = 1,  padding = 1)\n","    self.bn2 = nn.BatchNorm2d(30)\n","\n","  def forward(self, input):\n","\n","    x = F.relu(self.bn1(self.conv1(input)))\n","    x = F.relu(self.bn2(self.conv2(x)))\n","    # x = self.pool1(x)\n","    x = torch.reshape(x, (-1, 30*6*12))\n","\n","    return x\n","\n","\n","class Predictor(nn.Module):\n","\n","  def __init__(self):\n","    super(Predictor, self).__init__()\n","    self.fc1 = nn.Linear(30*6*12, 40*6)\n","\n","  def forward(self, input):\n","    pre = F.relu(self.fc1(input))\n","\n","    pre = pre.reshape(-1,40,6)\n","\n","    return pre\n","\n","class Domain_classifier(nn.Module):\n","\n","  def __init__(self):\n","    super(Domain_classifier, self).__init__()\n","    # self.fc1 = nn.Linear(50 * 4 * 4, 100)\n","    # self.bn1 = nn.BatchNorm1d(100)\n","    # self.fc2 = nn.Linear(100, 2)\n","    self.fc1 = nn.Linear(30*6*12, 1024)\n","    self.fc2 = nn.Linear(1024, 2)\n","\n","  def forward(self, input, constant):\n","    input = GradReverse.grad_reverse(input, constant)\n","    # logits = F.relu(self.bn1(self.fc1(input)))\n","    # logits = F.log_softmax(self.fc2(logits), 1)\n","    logits = F.relu(self.fc1(input))\n","\n","    logits = F.log_softmax(self.fc2(logits), dim = 1)\n","\n","    return logits\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"xrpKAdhBXjX7","executionInfo":{"status":"ok","timestamp":1715767272144,"user_tz":-120,"elapsed":5,"user":{"displayName":"J Lee","userId":"09182393954667450926"}}},"outputs":[],"source":["def train(training_mode, feature_extractor, class_classifier, domain_classifier, class_criterion, domain_criterion,\n","        source_dataloader1, source_dataloader2, source_dataloader3, source_dataloader4, target_dataloader,val_dataloader, optimizer, epoches):\n","  for epoch in range(epoches):\n","    early_stopping = EarlyStopping(patience=10, verbose=True)\n","    # track the losses as the model trains\n","    train_losses,valid_losses,avg_train_losses,avg_valid_losses = [],[],[],[]\n","    # steps\n","    start_steps = epoch * len(source_dataloader1)\n","    total_steps = 10 * len(source_dataloader1)\n","\n","    if training_mode == 'Pre-train':\n","      print('Pre-train Epoch: {}'.format(epoch))\n","      # setup models\n","      feature_extractor.train()\n","      class_classifier.train()\n","      domain_classifier.train()\n","      for batch_idx, (sdata1, sdata2, sdata3, sdata4, tdata) in enumerate(zip(source_dataloader1, source_dataloader2,\n","      source_dataloader3, source_dataloader4, target_dataloader)):\n","\n","        # setup hyperparameters\n","        p = float(batch_idx + start_steps) / total_steps\n","        constant = 2. / (1. + np.exp(-gamma * p)) - 1\n","\n","        # prepare the data\n","        input1s, label1s= sdata1\n","        input2s, label2s= sdata2\n","        input3s, label3s= sdata3\n","        input4s, label4s= sdata4\n","        input2, label2= tdata\n","\n","        size = min((input1s.shape[0], input2.shape[0]))\n","        input1s, label1s = input1s[0:size, :, :], label1s[0:size]\n","        input2s, label2s = input2s[0:size, :, :], label2s[0:size]\n","        input3s, label3s = input3s[0:size, :, :], label3s[0:size]\n","        input4s, label4s = input4s[0:size, :, :], label4s[0:size]\n","        input2, label2 = input2[0:size, :, :], label2[0:size]\n","\n","        input1s, label1s= Variable(input1s.to(device)), Variable(label1s.to(device).float())\n","        input2s, label2s= Variable(input2s.to(device)), Variable(label2s.to(device).float())\n","        input3s, label3s= Variable(input3s.to(device)), Variable(label3s.to(device).float())\n","        input4s, label4s= Variable(input4s.to(device)), Variable(label4s.to(device).float())\n","        input2, label2= Variable(input2.to(device)), Variable(label2.to(device).float())\n","\n","        # setup optimizer\n","        optimizer = optimizer_scheduler(optimizer, p)\n","        optimizer.zero_grad()\n","\n","        # prepare domain labels\n","        source_label1s = Variable(torch.zeros((input1s.size()[0])).type(torch.FloatTensor).to(device))\n","        source_label2s = Variable(torch.zeros((input2s.size()[0])).type(torch.FloatTensor).to(device))\n","        source_label3s = Variable(torch.zeros((input3s.size()[0])).type(torch.FloatTensor).to(device))\n","        source_label4s = Variable(torch.zeros((input4s.size()[0])).type(torch.FloatTensor).to(device))\n","\n","        target_labels = Variable(torch.ones((input2.size()[0])).type(torch.FloatTensor).to(device))\n","\n","        # compute the output of source domain and target domain\n","        src_feature1 = feature_extractor(input1s)\n","        src_feature2 = feature_extractor(input2s)\n","        src_feature3 = feature_extractor(input3s)\n","        src_feature4 = feature_extractor(input4s)\n","        tgt_feature = feature_extractor(input2)\n","\n","        # compute the loss of source & target preds\n","        class_pred1s = class_classifier(src_feature1)\n","        class_pred2s = class_classifier(src_feature2)\n","        class_pred3s = class_classifier(src_feature3)\n","        class_pred4s = class_classifier(src_feature4)\n","        class_predt = class_classifier(tgt_feature)\n","\n","        class_loss1s = class_criterion(class_pred1s, label1s)\n","        class_loss2s = class_criterion(class_pred2s, label2s)\n","        class_loss3s = class_criterion(class_pred3s, label3s)\n","        class_loss4s = class_criterion(class_pred4s, label4s)\n","        class_losst = class_criterion(class_predt, label2)\n","\n","        # compute the domain loss of src_feature and target_feature\n","        tgt_preds = domain_classifier(tgt_feature, constant)\n","        src_preds1 = domain_classifier(src_feature1, constant)\n","        src_preds2 = domain_classifier(src_feature2, constant)\n","        src_preds3 = domain_classifier(src_feature3, constant)\n","        src_preds4 = domain_classifier(src_feature4, constant)\n","\n","        tgt_loss = domain_criterion(tgt_preds.float(), target_labels.long())\n","        src_loss1 = domain_criterion(src_preds1.float(), source_label1s.long())\n","        src_loss2 = domain_criterion(src_preds2.float(), source_label2s.long())\n","        src_loss3 = domain_criterion(src_preds3.float(), source_label3s.long())\n","        src_loss4 = domain_criterion(src_preds4.float(), source_label4s.long())\n","\n","        domain_loss = 1*tgt_loss + 0.37*src_loss1 + 0.06*src_loss2 + 0.31*src_loss3 + 0.26*src_loss4\n","        class_loss = class_losst + 0.37*class_loss1s + 0.06*class_loss2s + 0.31*class_loss3s + 0.26*class_loss4s\n","\n","        # domain_loss = 1*tgt_loss + 1*src_loss1 + 0*src_loss2 + 1*src_loss3 + 0*src_loss4\n","        # class_loss = 1*class_losst + 1*class_loss1s + 0*class_loss2s + 1*class_loss3s + 0*class_loss4s\n","\n","        class_loss = class_loss/(5*batch_size*40*6)\n","\n","        loss = (1-theta)*class_loss + theta * domain_loss\n","\n","        loss=loss.float()\n","  #             print(loss.dtype)\n","        loss.backward()\n","        optimizer.step()\n","        train_losses.append(loss.item())\n","\n","        # print loss\n","        if (batch_idx + 1) % 10 == 0:\n","            print('[{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tPred Loss: {:.6f}\\tDomain Loss: {:.6f}'.format(\n","              batch_idx * len(input2), len(target_dataloader.dataset),\n","              100. * batch_idx / len(target_dataloader), loss.item(), class_loss.item(),\n","              domain_loss.item()\n","            ))\n","\n","            total_loss.append(loss.item())\n","            c_loss.append(class_loss.item())\n","            d_loss.append(domain_loss.item())\n","\n","\n","    elif training_mode == 'Fine-tune':\n","      print('FT Epoch: {}'.format(epoch))\n","      # setup models\n","      feature_extractor.train()\n","      class_classifier.train()\n","      domain_classifier.train()\n","      for batch_idx,tdata in enumerate(target_dataloader):\n","        p = float(batch_idx + start_steps) / total_steps\n","        constant = 2. / (1. + np.exp(-gamma * p)) - 1\n","        #prepare the target data\n","        input2, label2, = tdata\n","\n","        # print(input2.shape)\n","        # print(input2.type)\n","        input2, label2, = Variable(input2.to(device)), Variable(label2.to(device).float())\n","\n","        # setup optimizer\n","        optimizer = optimizer_scheduler(optimizer, p)\n","        optimizer.zero_grad()\n","\n","        # compute the output of target domain\n","        tgt_feature = feature_extractor(input2)\n","\n","        # compute the loss of target preds\n","        class_predt = class_classifier(tgt_feature)\n","\n","        class_losst = class_criterion(class_predt, label2)\n","\n","        loss = class_losst/(batch_size*40*6)\n","        loss=loss.float()\n","        loss.backward()\n","        optimizer.step()\n","        train_losses.append(loss.item())\n","\n","        # print loss\n","        if (batch_idx + 1) % 10 == 0:\n","          print('[({:.0f}%)]\\tPreds Loss: {:.6f}'.format(\n","            100. * batch_idx / len(target_dataloader), loss.item()))\n","      # val\n","      feature_extractor.eval()\n","      class_classifier.eval()\n","      for batch_idx,vdata in enumerate(val_dataloader):\n","        input_val, label_val, = vdata\n","        input_val, label_val, = Variable(input_val.to(device)), Variable(label_val.to(device).float())\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        tgt_feature_val = feature_extractor(input_val)\n","        # compute the loss of target preds\n","        class_pred_val = class_classifier(tgt_feature_val)\n","\n","        # calculate the loss\n","        loss_val = class_criterion(class_pred_val, label_val)\n","        # record validation loss\n","        valid_losses.append(loss_val.item())\n","\n","      train_loss = np.average(train_losses)\n","      valid_loss = np.average(valid_losses)\n","      avg_train_losses.append(train_loss)\n","      avg_valid_losses.append(valid_loss)\n","\n","      train_losses = []\n","      valid_losses = []\n","      # early_stopping needs the validation loss to check if it has decresed,\n","      # and if it has, it will make a checkpoint of the current model\n","      early_stopping(valid_loss, class_classifier)\n","\n","      if early_stopping.early_stop:\n","        print(\"Early stopping\")\n","        break\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"KzwrFYnUXjX8","executionInfo":{"status":"ok","timestamp":1715767272144,"user_tz":-120,"elapsed":4,"user":{"displayName":"J Lee","userId":"09182393954667450926"}}},"outputs":[],"source":["def test1(feature_extractor, class_classifier, domain_classifier,target_dataloader):\n","  \"\"\"\n","  Test the performance of the model\n","  :param feature_extractor: network used to extract feature from target samples\n","  :param class_classifier: network used to predict labels\n","  :param domain_classifier: network used to predict domain\n","  :param source_dataloader: test dataloader of source domain\n","  :param target_dataloader: test dataloader of target domain\n","  :return: None\n","  \"\"\"\n","  feature_extractor.eval()\n","  class_classifier.eval()\n","  domain_classifier.eval()\n","  target_correct = 0.0\n","  domain_correct = 0.0\n","  tgt_correct = 0.0\n","\n","  pred_list_t = []\n","  label_list_t = []\n","  for batch_idx, tdata in enumerate(target_dataloader):\n","    # setup hyperparameters\n","    p = float(batch_idx) / len(target_dataloader)\n","    constant = 2. / (1. + np.exp(-10 * p)) - 1\n","\n","    inputt, labelt= tdata\n","    inputt, labelt= Variable(inputt.to(device)), Variable(labelt.to(device).float())\n","    tgt_labels = Variable(torch.ones((inputt.size()[0])).type(torch.FloatTensor).to(device))\n","\n","    outputt = class_classifier(feature_extractor(inputt))\n","    predt = outputt\n","    pred_list_t.append(predt.tolist())\n","    label_list_t.append(labelt.tolist())\n","\n","    tgt_preds = domain_classifier(feature_extractor(inputt), constant)\n","    tgt_preds = tgt_preds.data.max(1, keepdim=True)[1]\n","    tgt_correct += tgt_preds.eq(tgt_labels.data.view_as(tgt_preds)).cuda().sum()\n","\n","\n","\n","  target_nrmse = nrmse_func2(pred_list_t, label_list_t)\n","  target_smape = smape_loss_func(pred_list_t, label_list_t)\n","  target_mae = mae_loss_func(pred_list_t, label_list_t)\n","\n","  print('\\nTarget error: {}/{}/{} \\n'.format(target_nrmse, target_smape, target_mae))\n","  t_nrmse_list.append(target_nrmse)\n","  t_smape_list.append(target_smape)\n","  t_mae_list.append(target_mae)\n","  return t_mae_list,t_smape_list,t_nrmse_list\n","  # np.save(file = r'.\\preds\\nrmse=%.4f, mae=%.4f, smape=%.4f' %(target_nrmse, target_mae, target_smape), arr = pred_list_t)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uNJ6-waJXjX9","outputId":"faa23c54-2a66-4766-94c4-221345b6fde6","scrolled":true,"executionInfo":{"status":"ok","timestamp":1715767337683,"user_tz":-120,"elapsed":65543,"user":{"displayName":"J Lee","userId":"09182393954667450926"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(3150, 40, 6)\n","data_process_time: 0.20160675048828125\n","Pre-train Epoch: 0\n","[576/3150 (18%)]\tLoss: 4.602628\tPred Loss: 4.702392\tDomain Loss: 2.707115\n","[1216/3150 (39%)]\tLoss: 2.572070\tPred Loss: 2.644782\tDomain Loss: 1.190552\n","[1856/3150 (59%)]\tLoss: 1.328858\tPred Loss: 1.341156\tDomain Loss: 1.095206\n","[2496/3150 (80%)]\tLoss: 0.934756\tPred Loss: 0.883136\tDomain Loss: 1.915540\n","Pre-train Epoch: 1\n","[576/3150 (18%)]\tLoss: 0.819038\tPred Loss: 0.784792\tDomain Loss: 1.469711\n","[1216/3150 (39%)]\tLoss: 0.774917\tPred Loss: 0.767699\tDomain Loss: 0.912075\n","[1856/3150 (59%)]\tLoss: 0.732610\tPred Loss: 0.750521\tDomain Loss: 0.392294\n","[2496/3150 (80%)]\tLoss: 0.724343\tPred Loss: 0.750838\tDomain Loss: 0.220928\n","Pre-train Epoch: 2\n","[576/3150 (18%)]\tLoss: 0.701919\tPred Loss: 0.727324\tDomain Loss: 0.219227\n","[1216/3150 (39%)]\tLoss: 0.702808\tPred Loss: 0.725537\tDomain Loss: 0.270954\n","[1856/3150 (59%)]\tLoss: 0.744447\tPred Loss: 0.745982\tDomain Loss: 0.715296\n","[2496/3150 (80%)]\tLoss: 0.670549\tPred Loss: 0.695849\tDomain Loss: 0.189838\n","Pre-train Epoch: 3\n","[576/3150 (18%)]\tLoss: 0.541249\tPred Loss: 0.550528\tDomain Loss: 0.364950\n","[1216/3150 (39%)]\tLoss: 0.491408\tPred Loss: 0.508162\tDomain Loss: 0.173070\n","[1856/3150 (59%)]\tLoss: 0.437499\tPred Loss: 0.459276\tDomain Loss: 0.023743\n","[2496/3150 (80%)]\tLoss: 0.407829\tPred Loss: 0.429077\tDomain Loss: 0.004126\n","Pre-train Epoch: 4\n","[576/3150 (18%)]\tLoss: 0.342262\tPred Loss: 0.360166\tDomain Loss: 0.002086\n","[1216/3150 (39%)]\tLoss: 0.317240\tPred Loss: 0.333909\tDomain Loss: 0.000526\n","[1856/3150 (59%)]\tLoss: 0.303201\tPred Loss: 0.319125\tDomain Loss: 0.000631\n","[2496/3150 (80%)]\tLoss: 0.296467\tPred Loss: 0.312039\tDomain Loss: 0.000595\n","Pre-train Epoch: 5\n","[576/3150 (18%)]\tLoss: 0.249196\tPred Loss: 0.262276\tDomain Loss: 0.000675\n","[1216/3150 (39%)]\tLoss: 0.269884\tPred Loss: 0.284082\tDomain Loss: 0.000125\n","[1856/3150 (59%)]\tLoss: 0.240029\tPred Loss: 0.252656\tDomain Loss: 0.000134\n","[2496/3150 (80%)]\tLoss: 0.254701\tPred Loss: 0.268100\tDomain Loss: 0.000111\n","Pre-train Epoch: 6\n","[576/3150 (18%)]\tLoss: 0.232093\tPred Loss: 0.244306\tDomain Loss: 0.000036\n","[1216/3150 (39%)]\tLoss: 0.219650\tPred Loss: 0.231205\tDomain Loss: 0.000105\n","[1856/3150 (59%)]\tLoss: 0.218914\tPred Loss: 0.230418\tDomain Loss: 0.000336\n","[2496/3150 (80%)]\tLoss: 0.236190\tPred Loss: 0.248619\tDomain Loss: 0.000041\n","Pre-train Epoch: 7\n","[576/3150 (18%)]\tLoss: 0.220516\tPred Loss: 0.232122\tDomain Loss: 0.000006\n","[1216/3150 (39%)]\tLoss: 0.212084\tPred Loss: 0.223246\tDomain Loss: 0.000005\n","[1856/3150 (59%)]\tLoss: 0.221875\tPred Loss: 0.233553\tDomain Loss: 0.000003\n","[2496/3150 (80%)]\tLoss: 0.214424\tPred Loss: 0.225709\tDomain Loss: 0.000006\n","Pre-train Epoch: 8\n","[576/3150 (18%)]\tLoss: 0.216707\tPred Loss: 0.228112\tDomain Loss: 0.000004\n","[1216/3150 (39%)]\tLoss: 0.204041\tPred Loss: 0.214779\tDomain Loss: 0.000018\n","[1856/3150 (59%)]\tLoss: 0.193818\tPred Loss: 0.204016\tDomain Loss: 0.000074\n","[2496/3150 (80%)]\tLoss: 0.197032\tPred Loss: 0.207402\tDomain Loss: 0.000005\n","Pre-train Epoch: 9\n","[576/3150 (18%)]\tLoss: 0.197339\tPred Loss: 0.207725\tDomain Loss: 0.000003\n","[1216/3150 (39%)]\tLoss: 0.191325\tPred Loss: 0.201395\tDomain Loss: 0.000002\n","[1856/3150 (59%)]\tLoss: 0.186133\tPred Loss: 0.195929\tDomain Loss: 0.000004\n","[2496/3150 (80%)]\tLoss: 0.188507\tPred Loss: 0.198428\tDomain Loss: 0.000013\n","Pre-train Epoch: 10\n","[576/3150 (18%)]\tLoss: 0.185668\tPred Loss: 0.195439\tDomain Loss: 0.000003\n","[1216/3150 (39%)]\tLoss: 0.192379\tPred Loss: 0.202504\tDomain Loss: 0.000005\n","[1856/3150 (59%)]\tLoss: 0.180836\tPred Loss: 0.190354\tDomain Loss: 0.000005\n","[2496/3150 (80%)]\tLoss: 0.183677\tPred Loss: 0.193344\tDomain Loss: 0.000003\n","Pre-train Epoch: 11\n","[576/3150 (18%)]\tLoss: 0.175939\tPred Loss: 0.185198\tDomain Loss: 0.000004\n","[1216/3150 (39%)]\tLoss: 0.179404\tPred Loss: 0.188845\tDomain Loss: 0.000018\n","[1856/3150 (59%)]\tLoss: 0.181188\tPred Loss: 0.190724\tDomain Loss: 0.000002\n","[2496/3150 (80%)]\tLoss: 0.177339\tPred Loss: 0.186672\tDomain Loss: 0.000005\n","Pre-train Epoch: 12\n","[576/3150 (18%)]\tLoss: 0.176529\tPred Loss: 0.185820\tDomain Loss: 0.000001\n","[1216/3150 (39%)]\tLoss: 0.169390\tPred Loss: 0.178304\tDomain Loss: 0.000008\n","[1856/3150 (59%)]\tLoss: 0.170519\tPred Loss: 0.179494\tDomain Loss: 0.000005\n","[2496/3150 (80%)]\tLoss: 0.173490\tPred Loss: 0.182621\tDomain Loss: 0.000003\n","Pre-train Epoch: 13\n","[576/3150 (18%)]\tLoss: 0.174397\tPred Loss: 0.183575\tDomain Loss: 0.000001\n","[1216/3150 (39%)]\tLoss: 0.172212\tPred Loss: 0.181276\tDomain Loss: 0.000001\n","[1856/3150 (59%)]\tLoss: 0.165427\tPred Loss: 0.174134\tDomain Loss: 0.000003\n","[2496/3150 (80%)]\tLoss: 0.168955\tPred Loss: 0.177848\tDomain Loss: 0.000000\n","Pre-train Epoch: 14\n","[576/3150 (18%)]\tLoss: 0.162383\tPred Loss: 0.170929\tDomain Loss: 0.000001\n","[1216/3150 (39%)]\tLoss: 0.163985\tPred Loss: 0.172616\tDomain Loss: 0.000001\n","[1856/3150 (59%)]\tLoss: 0.163204\tPred Loss: 0.171794\tDomain Loss: 0.000001\n","[2496/3150 (80%)]\tLoss: 0.158599\tPred Loss: 0.166946\tDomain Loss: 0.000001\n","Pre-train Epoch: 15\n","[576/3150 (18%)]\tLoss: 0.163579\tPred Loss: 0.172189\tDomain Loss: 0.000001\n","[1216/3150 (39%)]\tLoss: 0.160982\tPred Loss: 0.169454\tDomain Loss: 0.000001\n","[1856/3150 (59%)]\tLoss: 0.158638\tPred Loss: 0.166987\tDomain Loss: 0.000002\n","[2496/3150 (80%)]\tLoss: 0.160728\tPred Loss: 0.169187\tDomain Loss: 0.000002\n","Pre-train Epoch: 16\n","[576/3150 (18%)]\tLoss: 0.157377\tPred Loss: 0.165660\tDomain Loss: 0.000001\n","[1216/3150 (39%)]\tLoss: 0.155064\tPred Loss: 0.163225\tDomain Loss: 0.000001\n","[1856/3150 (59%)]\tLoss: 0.164425\tPred Loss: 0.173079\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.161777\tPred Loss: 0.170292\tDomain Loss: 0.000000\n","Pre-train Epoch: 17\n","[576/3150 (18%)]\tLoss: 0.163972\tPred Loss: 0.172602\tDomain Loss: 0.000001\n","[1216/3150 (39%)]\tLoss: 0.154637\tPred Loss: 0.162776\tDomain Loss: 0.000004\n","[1856/3150 (59%)]\tLoss: 0.157099\tPred Loss: 0.165367\tDomain Loss: 0.000001\n","[2496/3150 (80%)]\tLoss: 0.156641\tPred Loss: 0.164885\tDomain Loss: 0.000001\n","Pre-train Epoch: 18\n","[576/3150 (18%)]\tLoss: 0.153033\tPred Loss: 0.161088\tDomain Loss: 0.000001\n","[1216/3150 (39%)]\tLoss: 0.155052\tPred Loss: 0.163213\tDomain Loss: 0.000002\n","[1856/3150 (59%)]\tLoss: 0.161734\tPred Loss: 0.170246\tDomain Loss: 0.000001\n","[2496/3150 (80%)]\tLoss: 0.150901\tPred Loss: 0.158843\tDomain Loss: 0.000000\n","Pre-train Epoch: 19\n","[576/3150 (18%)]\tLoss: 0.157440\tPred Loss: 0.165726\tDomain Loss: 0.000001\n","[1216/3150 (39%)]\tLoss: 0.150524\tPred Loss: 0.158447\tDomain Loss: 0.000001\n","[1856/3150 (59%)]\tLoss: 0.149706\tPred Loss: 0.157586\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.149344\tPred Loss: 0.157204\tDomain Loss: 0.000001\n","Pre-train Epoch: 20\n","[576/3150 (18%)]\tLoss: 0.138958\tPred Loss: 0.146272\tDomain Loss: 0.000001\n","[1216/3150 (39%)]\tLoss: 0.147886\tPred Loss: 0.155670\tDomain Loss: 0.000001\n","[1856/3150 (59%)]\tLoss: 0.141087\tPred Loss: 0.148513\tDomain Loss: 0.000001\n","[2496/3150 (80%)]\tLoss: 0.141563\tPred Loss: 0.149013\tDomain Loss: 0.000000\n","Pre-train Epoch: 21\n","[576/3150 (18%)]\tLoss: 0.144202\tPred Loss: 0.151791\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.141450\tPred Loss: 0.148895\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.141789\tPred Loss: 0.149251\tDomain Loss: 0.000002\n","[2496/3150 (80%)]\tLoss: 0.146129\tPred Loss: 0.153820\tDomain Loss: 0.000001\n","Pre-train Epoch: 22\n","[576/3150 (18%)]\tLoss: 0.141294\tPred Loss: 0.148730\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.143098\tPred Loss: 0.150629\tDomain Loss: 0.000001\n","[1856/3150 (59%)]\tLoss: 0.149764\tPred Loss: 0.157646\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.143178\tPred Loss: 0.150714\tDomain Loss: 0.000003\n","Pre-train Epoch: 23\n","[576/3150 (18%)]\tLoss: 0.145644\tPred Loss: 0.153310\tDomain Loss: 0.000001\n","[1216/3150 (39%)]\tLoss: 0.143494\tPred Loss: 0.151046\tDomain Loss: 0.000001\n","[1856/3150 (59%)]\tLoss: 0.138838\tPred Loss: 0.146145\tDomain Loss: 0.000001\n","[2496/3150 (80%)]\tLoss: 0.141768\tPred Loss: 0.149230\tDomain Loss: 0.000001\n","Pre-train Epoch: 24\n","[576/3150 (18%)]\tLoss: 0.148046\tPred Loss: 0.155838\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.141271\tPred Loss: 0.148706\tDomain Loss: 0.000001\n","[1856/3150 (59%)]\tLoss: 0.131551\tPred Loss: 0.138475\tDomain Loss: 0.000001\n","[2496/3150 (80%)]\tLoss: 0.138379\tPred Loss: 0.145662\tDomain Loss: 0.000001\n","Pre-train Epoch: 25\n","[576/3150 (18%)]\tLoss: 0.130581\tPred Loss: 0.137453\tDomain Loss: 0.000005\n","[1216/3150 (39%)]\tLoss: 0.140169\tPred Loss: 0.147547\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.141554\tPred Loss: 0.149004\tDomain Loss: 0.000005\n","[2496/3150 (80%)]\tLoss: 0.137701\tPred Loss: 0.144948\tDomain Loss: 0.000000\n","Pre-train Epoch: 26\n","[576/3150 (18%)]\tLoss: 0.131037\tPred Loss: 0.137934\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.128872\tPred Loss: 0.135655\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.129376\tPred Loss: 0.136185\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.136200\tPred Loss: 0.143369\tDomain Loss: 0.000001\n","Pre-train Epoch: 27\n","[576/3150 (18%)]\tLoss: 0.131211\tPred Loss: 0.138117\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.132869\tPred Loss: 0.139862\tDomain Loss: 0.000001\n","[1856/3150 (59%)]\tLoss: 0.129463\tPred Loss: 0.136277\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.126829\tPred Loss: 0.133504\tDomain Loss: 0.000000\n","Pre-train Epoch: 28\n","[576/3150 (18%)]\tLoss: 0.130677\tPred Loss: 0.137554\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.131690\tPred Loss: 0.138621\tDomain Loss: 0.000001\n","[1856/3150 (59%)]\tLoss: 0.132841\tPred Loss: 0.139832\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.138559\tPred Loss: 0.145852\tDomain Loss: 0.000000\n","Pre-train Epoch: 29\n","[576/3150 (18%)]\tLoss: 0.129925\tPred Loss: 0.136763\tDomain Loss: 0.000001\n","[1216/3150 (39%)]\tLoss: 0.129762\tPred Loss: 0.136592\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.129113\tPred Loss: 0.135908\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.128320\tPred Loss: 0.135073\tDomain Loss: 0.000000\n","Pre-train Epoch: 30\n","[576/3150 (18%)]\tLoss: 0.129113\tPred Loss: 0.135908\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.126757\tPred Loss: 0.133429\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.126952\tPred Loss: 0.133633\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.134176\tPred Loss: 0.141238\tDomain Loss: 0.000001\n","Pre-train Epoch: 31\n","[576/3150 (18%)]\tLoss: 0.120505\tPred Loss: 0.126847\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.130527\tPred Loss: 0.137397\tDomain Loss: 0.000001\n","[1856/3150 (59%)]\tLoss: 0.122632\tPred Loss: 0.129086\tDomain Loss: 0.000001\n","[2496/3150 (80%)]\tLoss: 0.128025\tPred Loss: 0.134763\tDomain Loss: 0.000000\n","Pre-train Epoch: 32\n","[576/3150 (18%)]\tLoss: 0.124011\tPred Loss: 0.130538\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.127400\tPred Loss: 0.134106\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.133096\tPred Loss: 0.140101\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.123259\tPred Loss: 0.129746\tDomain Loss: 0.000000\n","Pre-train Epoch: 33\n","[576/3150 (18%)]\tLoss: 0.132488\tPred Loss: 0.139461\tDomain Loss: 0.000002\n","[1216/3150 (39%)]\tLoss: 0.123630\tPred Loss: 0.130137\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.130491\tPred Loss: 0.137359\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.126535\tPred Loss: 0.133195\tDomain Loss: 0.000000\n","Pre-train Epoch: 34\n","[576/3150 (18%)]\tLoss: 0.122889\tPred Loss: 0.129357\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.121699\tPred Loss: 0.128104\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.120411\tPred Loss: 0.126748\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.126793\tPred Loss: 0.133466\tDomain Loss: 0.000000\n","Pre-train Epoch: 35\n","[576/3150 (18%)]\tLoss: 0.130212\tPred Loss: 0.137065\tDomain Loss: 0.000001\n","[1216/3150 (39%)]\tLoss: 0.117763\tPred Loss: 0.123961\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.121450\tPred Loss: 0.127842\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.124931\tPred Loss: 0.131506\tDomain Loss: 0.000000\n","Pre-train Epoch: 36\n","[576/3150 (18%)]\tLoss: 0.120611\tPred Loss: 0.126959\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.118217\tPred Loss: 0.124439\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.119404\tPred Loss: 0.125689\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.122696\tPred Loss: 0.129154\tDomain Loss: 0.000000\n","Pre-train Epoch: 37\n","[576/3150 (18%)]\tLoss: 0.123223\tPred Loss: 0.129708\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.117830\tPred Loss: 0.124032\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.124322\tPred Loss: 0.130865\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.124988\tPred Loss: 0.131566\tDomain Loss: 0.000001\n","Pre-train Epoch: 38\n","[576/3150 (18%)]\tLoss: 0.123006\tPred Loss: 0.129480\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.119565\tPred Loss: 0.125858\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.127539\tPred Loss: 0.134252\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.124643\tPred Loss: 0.131203\tDomain Loss: 0.000000\n","Pre-train Epoch: 39\n","[576/3150 (18%)]\tLoss: 0.128273\tPred Loss: 0.135024\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.117806\tPred Loss: 0.124006\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.124302\tPred Loss: 0.130845\tDomain Loss: 0.000001\n","[2496/3150 (80%)]\tLoss: 0.118366\tPred Loss: 0.124596\tDomain Loss: 0.000000\n","Pre-train Epoch: 40\n","[576/3150 (18%)]\tLoss: 0.119689\tPred Loss: 0.125988\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.117495\tPred Loss: 0.123679\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.119205\tPred Loss: 0.125478\tDomain Loss: 0.000001\n","[2496/3150 (80%)]\tLoss: 0.112204\tPred Loss: 0.118110\tDomain Loss: 0.000000\n","Pre-train Epoch: 41\n","[576/3150 (18%)]\tLoss: 0.107945\tPred Loss: 0.113626\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.123308\tPred Loss: 0.129798\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.117660\tPred Loss: 0.123853\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.114059\tPred Loss: 0.120062\tDomain Loss: 0.000000\n","Pre-train Epoch: 42\n","[576/3150 (18%)]\tLoss: 0.112023\tPred Loss: 0.117919\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.111374\tPred Loss: 0.117236\tDomain Loss: 0.000001\n","[1856/3150 (59%)]\tLoss: 0.122779\tPred Loss: 0.129241\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.120197\tPred Loss: 0.126523\tDomain Loss: 0.000000\n","Pre-train Epoch: 43\n","[576/3150 (18%)]\tLoss: 0.113241\tPred Loss: 0.119201\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.112485\tPred Loss: 0.118405\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.112567\tPred Loss: 0.118492\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.115053\tPred Loss: 0.121109\tDomain Loss: 0.000000\n","Pre-train Epoch: 44\n","[576/3150 (18%)]\tLoss: 0.111838\tPred Loss: 0.117725\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.115676\tPred Loss: 0.121765\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.124098\tPred Loss: 0.130629\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.117758\tPred Loss: 0.123956\tDomain Loss: 0.000000\n","Pre-train Epoch: 45\n","[576/3150 (18%)]\tLoss: 0.112925\tPred Loss: 0.118868\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.108446\tPred Loss: 0.114154\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.108751\tPred Loss: 0.114474\tDomain Loss: 0.000001\n","[2496/3150 (80%)]\tLoss: 0.110729\tPred Loss: 0.116556\tDomain Loss: 0.000000\n","Pre-train Epoch: 46\n","[576/3150 (18%)]\tLoss: 0.114336\tPred Loss: 0.120354\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.111138\tPred Loss: 0.116987\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.109800\tPred Loss: 0.115579\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.114707\tPred Loss: 0.120744\tDomain Loss: 0.000000\n","Pre-train Epoch: 47\n","[576/3150 (18%)]\tLoss: 0.111039\tPred Loss: 0.116883\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.115477\tPred Loss: 0.121555\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.116938\tPred Loss: 0.123093\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.115771\tPred Loss: 0.121864\tDomain Loss: 0.000000\n","Pre-train Epoch: 48\n","[576/3150 (18%)]\tLoss: 0.117720\tPred Loss: 0.123916\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.113679\tPred Loss: 0.119662\tDomain Loss: 0.000001\n","[1856/3150 (59%)]\tLoss: 0.115289\tPred Loss: 0.121357\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.113256\tPred Loss: 0.119217\tDomain Loss: 0.000000\n","Pre-train Epoch: 49\n","[576/3150 (18%)]\tLoss: 0.106080\tPred Loss: 0.111663\tDomain Loss: 0.000000\n","[1216/3150 (39%)]\tLoss: 0.112048\tPred Loss: 0.117945\tDomain Loss: 0.000000\n","[1856/3150 (59%)]\tLoss: 0.113880\tPred Loss: 0.119873\tDomain Loss: 0.000000\n","[2496/3150 (80%)]\tLoss: 0.112126\tPred Loss: 0.118028\tDomain Loss: 0.000000\n","\n","Target error: 0.1801418797106366/0.10218234229537354/66.88218493767835 \n","\n","FT Epoch: 0\n","[(18%)]\tPreds Loss: 0.731937\n","[(39%)]\tPreds Loss: 0.571002\n","[(59%)]\tPreds Loss: 0.782148\n","[(80%)]\tPreds Loss: 0.556381\n","Saving model ...\n","FT Epoch: 1\n","[(18%)]\tPreds Loss: 0.462632\n","[(39%)]\tPreds Loss: 0.388713\n","[(59%)]\tPreds Loss: 0.455057\n","[(80%)]\tPreds Loss: 0.409947\n","Saving model ...\n","FT Epoch: 2\n","[(18%)]\tPreds Loss: 0.430903\n","[(39%)]\tPreds Loss: 0.390438\n","[(59%)]\tPreds Loss: 0.375602\n","[(80%)]\tPreds Loss: 0.385906\n","Saving model ...\n","FT Epoch: 3\n","[(18%)]\tPreds Loss: 0.432692\n","[(39%)]\tPreds Loss: 0.469393\n","[(59%)]\tPreds Loss: 0.410880\n","[(80%)]\tPreds Loss: 0.378165\n","Saving model ...\n","FT Epoch: 4\n","[(18%)]\tPreds Loss: 0.355223\n","[(39%)]\tPreds Loss: 0.430756\n","[(59%)]\tPreds Loss: 0.364359\n","[(80%)]\tPreds Loss: 0.346111\n","Saving model ...\n","FT Epoch: 5\n","[(18%)]\tPreds Loss: 0.356639\n","[(39%)]\tPreds Loss: 0.338705\n","[(59%)]\tPreds Loss: 0.328593\n","[(80%)]\tPreds Loss: 0.370061\n","Saving model ...\n","FT Epoch: 6\n","[(18%)]\tPreds Loss: 0.385803\n","[(39%)]\tPreds Loss: 0.328678\n","[(59%)]\tPreds Loss: 0.309983\n","[(80%)]\tPreds Loss: 0.294642\n","Saving model ...\n","FT Epoch: 7\n","[(18%)]\tPreds Loss: 0.343861\n","[(39%)]\tPreds Loss: 0.366858\n","[(59%)]\tPreds Loss: 0.278764\n","[(80%)]\tPreds Loss: 0.353122\n","Saving model ...\n","FT Epoch: 8\n","[(18%)]\tPreds Loss: 0.339238\n","[(39%)]\tPreds Loss: 0.277520\n","[(59%)]\tPreds Loss: 0.289249\n","[(80%)]\tPreds Loss: 0.325992\n","Saving model ...\n","FT Epoch: 9\n","[(18%)]\tPreds Loss: 0.274802\n","[(39%)]\tPreds Loss: 0.278135\n","[(59%)]\tPreds Loss: 0.267078\n","[(80%)]\tPreds Loss: 0.245574\n","Saving model ...\n","FT Epoch: 10\n","[(18%)]\tPreds Loss: 0.298271\n","[(39%)]\tPreds Loss: 0.279775\n","[(59%)]\tPreds Loss: 0.304441\n","[(80%)]\tPreds Loss: 0.273679\n","Saving model ...\n","FT Epoch: 11\n","[(18%)]\tPreds Loss: 0.258915\n","[(39%)]\tPreds Loss: 0.262014\n","[(59%)]\tPreds Loss: 0.259546\n","[(80%)]\tPreds Loss: 0.296921\n","Saving model ...\n","FT Epoch: 12\n","[(18%)]\tPreds Loss: 0.285873\n","[(39%)]\tPreds Loss: 0.254362\n","[(59%)]\tPreds Loss: 0.254619\n","[(80%)]\tPreds Loss: 0.257941\n","Saving model ...\n","FT Epoch: 13\n","[(18%)]\tPreds Loss: 0.281138\n","[(39%)]\tPreds Loss: 0.241244\n","[(59%)]\tPreds Loss: 0.279196\n","[(80%)]\tPreds Loss: 0.244044\n","Saving model ...\n","FT Epoch: 14\n","[(18%)]\tPreds Loss: 0.247488\n","[(39%)]\tPreds Loss: 0.262924\n","[(59%)]\tPreds Loss: 0.261352\n","[(80%)]\tPreds Loss: 0.279201\n","Saving model ...\n","FT Epoch: 15\n","[(18%)]\tPreds Loss: 0.252419\n","[(39%)]\tPreds Loss: 0.246593\n","[(59%)]\tPreds Loss: 0.248409\n","[(80%)]\tPreds Loss: 0.231496\n","Saving model ...\n","FT Epoch: 16\n","[(18%)]\tPreds Loss: 0.241700\n","[(39%)]\tPreds Loss: 0.242776\n","[(59%)]\tPreds Loss: 0.256615\n","[(80%)]\tPreds Loss: 0.254028\n","Saving model ...\n","FT Epoch: 17\n","[(18%)]\tPreds Loss: 0.270870\n","[(39%)]\tPreds Loss: 0.250518\n","[(59%)]\tPreds Loss: 0.243572\n","[(80%)]\tPreds Loss: 0.217072\n","Saving model ...\n","FT Epoch: 18\n","[(18%)]\tPreds Loss: 0.247595\n","[(39%)]\tPreds Loss: 0.233999\n","[(59%)]\tPreds Loss: 0.244295\n","[(80%)]\tPreds Loss: 0.232020\n","Saving model ...\n","FT Epoch: 19\n","[(18%)]\tPreds Loss: 0.232493\n","[(39%)]\tPreds Loss: 0.335433\n","[(59%)]\tPreds Loss: 0.253618\n","[(80%)]\tPreds Loss: 0.247434\n","Saving model ...\n","FT Epoch: 20\n","[(18%)]\tPreds Loss: 0.235529\n","[(39%)]\tPreds Loss: 0.246115\n","[(59%)]\tPreds Loss: 0.223536\n","[(80%)]\tPreds Loss: 0.231413\n","Saving model ...\n","FT Epoch: 21\n","[(18%)]\tPreds Loss: 0.221473\n","[(39%)]\tPreds Loss: 0.239111\n","[(59%)]\tPreds Loss: 0.247806\n","[(80%)]\tPreds Loss: 0.209684\n","Saving model ...\n","FT Epoch: 22\n","[(18%)]\tPreds Loss: 0.214145\n","[(39%)]\tPreds Loss: 0.232308\n","[(59%)]\tPreds Loss: 0.218600\n","[(80%)]\tPreds Loss: 0.223065\n","Saving model ...\n","FT Epoch: 23\n","[(18%)]\tPreds Loss: 0.231924\n","[(39%)]\tPreds Loss: 0.231975\n","[(59%)]\tPreds Loss: 0.245848\n","[(80%)]\tPreds Loss: 0.230774\n","Saving model ...\n","FT Epoch: 24\n","[(18%)]\tPreds Loss: 0.220199\n","[(39%)]\tPreds Loss: 0.232855\n","[(59%)]\tPreds Loss: 0.203774\n","[(80%)]\tPreds Loss: 0.204977\n","Saving model ...\n","FT Epoch: 25\n","[(18%)]\tPreds Loss: 0.255448\n","[(39%)]\tPreds Loss: 0.235181\n","[(59%)]\tPreds Loss: 0.200679\n","[(80%)]\tPreds Loss: 0.229252\n","Saving model ...\n","FT Epoch: 26\n","[(18%)]\tPreds Loss: 0.216249\n","[(39%)]\tPreds Loss: 0.218574\n","[(59%)]\tPreds Loss: 0.217272\n","[(80%)]\tPreds Loss: 0.225582\n","Saving model ...\n","FT Epoch: 27\n","[(18%)]\tPreds Loss: 0.217806\n","[(39%)]\tPreds Loss: 0.206947\n","[(59%)]\tPreds Loss: 0.205041\n","[(80%)]\tPreds Loss: 0.217076\n","Saving model ...\n","FT Epoch: 28\n","[(18%)]\tPreds Loss: 0.215779\n","[(39%)]\tPreds Loss: 0.211558\n","[(59%)]\tPreds Loss: 0.227903\n","[(80%)]\tPreds Loss: 0.188414\n","Saving model ...\n","FT Epoch: 29\n","[(18%)]\tPreds Loss: 0.231279\n","[(39%)]\tPreds Loss: 0.212156\n","[(59%)]\tPreds Loss: 0.221200\n","[(80%)]\tPreds Loss: 0.224976\n","Saving model ...\n","FT Epoch: 30\n","[(18%)]\tPreds Loss: 0.216382\n","[(39%)]\tPreds Loss: 0.209350\n","[(59%)]\tPreds Loss: 0.213789\n","[(80%)]\tPreds Loss: 0.190606\n","Saving model ...\n","FT Epoch: 31\n","[(18%)]\tPreds Loss: 0.207242\n","[(39%)]\tPreds Loss: 0.204412\n","[(59%)]\tPreds Loss: 0.205479\n","[(80%)]\tPreds Loss: 0.198556\n","Saving model ...\n","FT Epoch: 32\n","[(18%)]\tPreds Loss: 0.203872\n","[(39%)]\tPreds Loss: 0.224387\n","[(59%)]\tPreds Loss: 0.228524\n","[(80%)]\tPreds Loss: 0.217726\n","Saving model ...\n","FT Epoch: 33\n","[(18%)]\tPreds Loss: 0.206999\n","[(39%)]\tPreds Loss: 0.227028\n","[(59%)]\tPreds Loss: 0.210496\n","[(80%)]\tPreds Loss: 0.193908\n","Saving model ...\n","FT Epoch: 34\n","[(18%)]\tPreds Loss: 0.212509\n","[(39%)]\tPreds Loss: 0.210659\n","[(59%)]\tPreds Loss: 0.222128\n","[(80%)]\tPreds Loss: 0.194607\n","Saving model ...\n","FT Epoch: 35\n","[(18%)]\tPreds Loss: 0.198698\n","[(39%)]\tPreds Loss: 0.210101\n","[(59%)]\tPreds Loss: 0.217057\n","[(80%)]\tPreds Loss: 0.187165\n","Saving model ...\n","FT Epoch: 36\n","[(18%)]\tPreds Loss: 0.196839\n","[(39%)]\tPreds Loss: 0.218603\n","[(59%)]\tPreds Loss: 0.203973\n","[(80%)]\tPreds Loss: 0.194000\n","Saving model ...\n","FT Epoch: 37\n","[(18%)]\tPreds Loss: 0.194726\n","[(39%)]\tPreds Loss: 0.201935\n","[(59%)]\tPreds Loss: 0.185271\n","[(80%)]\tPreds Loss: 0.200516\n","Saving model ...\n","FT Epoch: 38\n","[(18%)]\tPreds Loss: 0.210250\n","[(39%)]\tPreds Loss: 0.203044\n","[(59%)]\tPreds Loss: 0.191337\n","[(80%)]\tPreds Loss: 0.200595\n","Saving model ...\n","FT Epoch: 39\n","[(18%)]\tPreds Loss: 0.179965\n","[(39%)]\tPreds Loss: 0.202309\n","[(59%)]\tPreds Loss: 0.195650\n","[(80%)]\tPreds Loss: 0.193768\n","Saving model ...\n","\n","Target error: 0.1566741340286083/0.08742080807210728/55.004804691094485 \n","\n","pretraining time: 46.36238479614258\n","finetuning time: 9.569782495498657\n","total run time: (min) 0.9902362823486328\n"]}],"source":["def main():\n","  # prepare the source data and target data\n","  image_train_s1, image_test_s1, label_train_s1, label_test_s1, image_train_s2, image_test_s2, label_train_s2, label_test_s2, image_train_s3, image_test_s3, label_train_s3, label_test_s3, image_train_s4, image_test_s4, label_train_s4, label_test_s4, image_train_t,image_test_t,label_train_t,label_test_t,test,label_test= time_slide(time_slide1s=0/31, time_slide1t=22/31, time_slide2=25/31)\n","\n","  print(label_train_s1.shape)\n","  time01 = time.time()\n","  val_num=144\n","  src_train_dataloader1 = get_train_loader(image_train_s1,label_train_s1,batch_size=batch_size,shuffle=True)\n","  src_test_dataloader1 = get_test_loader(image_test_s1,label_test_s1,batch_size=batch_size,shuffle=True)\n","  src_train_dataloader2 = get_train_loader(image_train_s2,label_train_s2,batch_size=batch_size,shuffle=True)\n","  src_test_dataloader2 = get_test_loader(image_test_s2,label_test_s2,batch_size=batch_size,shuffle=True)\n","  src_train_dataloader3 = get_train_loader(image_train_s3,label_train_s3,batch_size=batch_size,shuffle=True)\n","  src_test_dataloader3 = get_test_loader(image_test_s3,label_test_s3,batch_size=batch_size,shuffle=True)\n","  src_train_dataloader4 = get_train_loader(image_train_s4,label_train_s4,batch_size=batch_size,shuffle=True)\n","  src_test_dataloader4 = get_test_loader(image_test_s4,label_test_s4,batch_size=batch_size,shuffle=True)\n","\n","  tgt_train_dataloader = get_train_loader(image_train_t,label_train_t,batch_size=batch_size,shuffle=True)\n","  tgt_test_dataloader = get_test_loader(image_test_t,label_test_t,batch_size=batch_size,shuffle=True)\n","  val_train_dataloader = get_train_loader(image_test_t[:val_num,:,:,:],label_test_t[:val_num,:,:],batch_size=batch_size,shuffle=True)\n","\n","  time02 = time.time()\n","\n","  print('data_process_time: '+ str (time02-time01))\n","\n","  # init models\n","  feature_extractor = Extractor().to(device)\n","  class_classifier = Predictor().to(device)\n","  domain_classifier = Domain_classifier().to(device)\n","\n","  # init criterions\n","  class_criterion = nn.MSELoss().to(device)\n","  # domain_criterion = torch.nn.CrossEntropyLoss().to(device)\n","  domain_criterion = nn.NLLLoss()\n","\n","  # init optimizer\n","  optimizer = optim.Adam([\n","          {'params': feature_extractor.parameters()},\n","                          {'params': class_classifier.parameters()},\n","                          {'params': domain_classifier.parameters()}\n","  ], lr= 0.001)\n","\n","  time1 = time.time()\n","\n","  train('Pre-train', feature_extractor, class_classifier, domain_classifier, class_criterion, domain_criterion,\n","    src_train_dataloader1, src_train_dataloader2, src_train_dataloader3, src_train_dataloader4, tgt_train_dataloader,val_train_dataloader, optimizer, epoches=50)\n","  test1(feature_extractor, class_classifier, domain_classifier,tgt_test_dataloader)\n","\n","  time2 = time.time()\n","  train('Fine-tune', feature_extractor, class_classifier, domain_classifier, class_criterion, domain_criterion,\n","    src_train_dataloader1, src_train_dataloader2, src_train_dataloader3, src_train_dataloader4, tgt_train_dataloader,val_train_dataloader, optimizer, epoches=40)\n","  test1(feature_extractor, class_classifier, domain_classifier,tgt_test_dataloader)\n","\n","  time3 = time.time()\n","  print('pretraining time: ' + str(time2-time1))\n","  print('finetuning time: ' + str(time3-time2))\n","\n","device = torch.device(\"cuda:0\")\n","total_loss, d_loss, c_loss = [],[],[]\n","s1_nrmse_list, s2_nrmse_list, s3_nrmse_list, s4_nrmse_list, t_nrmse_list, domain_loss_list = [],[],[],[],[],[]\n","s1_smape_list, s2_smape_list, s3_smape_list, s4_smape_list, t_smape_list = [],[],[],[],[]\n","s1_mae_list, s2_mae_list, s3_mae_list, s4_mae_list, t_mae_list = [],[],[],[],[]\n","near_road_target = np.argsort(np.array(pd.read_csv('./data/dis_blue.csv',header = None)))\n","flow_target = np.array(pd.read_csv('./data/flow_blue.csv', header= 0))\n","\n","near_road_source1 = np.argsort(np.array(pd.read_csv('./data/dis_green.csv',header = None)))\n","flow_source1 = np.array(pd.read_csv('./data/flow_green.csv', header= 0))\n","\n","near_road_source2 = np.argsort(np.array(pd.read_csv('./data/dis_yellow.csv',header = None)))\n","flow_source2 = np.array(pd.read_csv('./data/flow_yellow.csv', header= 0))\n","\n","near_road_source3 = np.argsort(np.array(pd.read_csv('./data/dis_purple.csv',header = None)))\n","flow_source3 = np.array(pd.read_csv('./data/flow_purple.csv', header= 0))\n","\n","near_road_source4 = np.argsort(np.array(pd.read_csv('./data/dis_red.csv',header = None)))\n","flow_source4 = np.array(pd.read_csv('./data/flow_red.csv', header= 0))\n","# pred_list_s1, pred_list_s2, pred_list_s3, pred_list_s4, pred_list_t = [],[],[],[],[]\n","# label_list_s1, label_list_s2, label_list_s3, label_list_s4, label_list_t = [],[],[],[],[]\n","\n","if __name__ == '__main__':\n","  setup_seed(0)\n","  target_links=40\n","  gamma = 5\n","  theta = 0.05\n","  batch_size = 64\n","  time_start=time.time()\n","  main()\n","  time_end=time.time()\n","  print('total run time: (min)',(time_end-time_start)/60.)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","interpreter":{"hash":"ede0b0c248aff89f28ae18209387c18b6576badc4d44ab9abec62ea9812901de"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"vscode":{"interpreter":{"hash":"ede0b0c248aff89f28ae18209387c18b6576badc4d44ab9abec62ea9812901de"}}},"nbformat":4,"nbformat_minor":0}